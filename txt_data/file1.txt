 CAT-3 2127200801094 Thanigaikelann IT 18305 Database Management INT-B systeri 8 b The deadlock - prevention and deadlock. detection algrithms can be used in a distributed system, provided that modification are Made. FBI example, we can use the tree protocol by defining a global tree among the system data items. similarly, the tirestamp - ordering approach could be directly applied to a distributed environment Deadlock prevention May result in unnecessary waiting and rollburs furthersta, certain dealbock preventson technique May requires more site to be involved in the execution of a transaction than would otherwise be the case. If me allow deadlocks to occur and on deadlock detection, the Main problem in a distributed reply system is deciding how to maintain the wait for graphs. T, T2 T2 T4 T5 T3 T3 COMMON techniques for dealing with this issue require that each site keep a local wait for graph The node of the graph correspond to all the transactions (local as well as noulocar) that are Currently either holding or requesting any of the items local to that site. Example: depices a system consisting of two sites each maintaining its local wait - for graph Note the transactions T2 and T3 appear in both graphs, indicating that the transactions have requested items as both sites when a transaction Ti on site S, needs a resource in site S2, it sends a request Message to site S2. If the resource is held by transaction Tj, the system insert an edges T; T in the local wast- for graph of site S2 In the centralized deadlock detection approach the system constructs and maintains a global wastfor graph in a single site the deadlock-detection - coordinator since there is communication delay in the system, we must distinguish between two types of wait - for graphs. The real graph describes the real but unknown state of the system at any instance in time, as would be seen by an omnisciend observer. T1 T1 T2 T3 S1 S2 T1 T2 T3 coordinator The global wait for graph can be reconstruct ted or updated under the conditions. - whenever a new edge is inserted in or removed from one of the local wait for graphs periodically, when a number of changes have occurred in a local wait for graph. whenever the coordinator needs 10 invoke the cycle- detection algorithm False cycles exist in the global mont for graph As am illustration, consider a snapshor of the system represented by the local wait for graph suppose that T2 redease the resource that is holding in site S1, resulting in the delection of the edge T, T2 in S1. Transaction T2 then request a resource held by T3 at site S2, resulting in the - addition of the edges T2 T3 ins, If the insert T2 T3 message from S2 arrius before the remone T, T2 Message from S., then coordinator May discover the false cycle T, T2 Ts after the insert. A deadlock has indeed occured and a victim has been picked, while one of the transactions was aborted for reasons unrelated to the deadlock For example, suppose that site S, decides to about T2. At the same time, the coordinator has discovered a cycle and was picked T3 as a victim Both T2. and T3 are now rolled back, although only T2 needed f 0 be rolled back part- 6 b Executive overreven The requirement for high data availability and the need to access data or hear 24/7/365 without performance degradation and service interruption has created the need for having redundant distributed copes of the data However in today's complex It environment where data increases in light speed, maintaining data consistency across distributed copies of data is challenging and the possibility of data discripary is an unfortunate reality oracle gddengate veredata provides an easy - to - use get powerful solution for identifying out of synch data before it negatively impacts the business. Deployed together with the oracle Goldengate real-time - data replication product or separately, oracle goldengate vridada data consistency is maintained across databases ensures challenges in maintaining Data consistancy byfre we discuss the requirements for the solution that helps Manage data consestancy across database, we need to understand the COMMON causes of data discrepancies in an enterprese Data discrepancy occurs when the data in the target database desiates from the source database The extent to which the data deviates depends on various fallors, some of which May be intended and others instended some of the potential causes of data Discripancy are described in the following sections migration error Different kind of Migration tools are empty to faciliate the initial load of the starget databa before replication can begin Differences in configurate for handing data by the migration tools and replica products can result in data discrepancies For example. a migration tool May use"?" and the replication product May use Null" when the value of the column is known. lift & shift worload to cloud since the world is Moving towards cloud, the lift & shift of database workload from on premises to cloud is the need of todays IT world racle gddengate helps moving the work load, the data consistency across on - premises and cloud data Difference in source and Target Different including, locates, endianner or database versions can Cause rubte descriputions to happen during migration and replication instantiation cross world migration of replication can begin the target database will need to be instantiated with the correct schema and constraints Failure to do so will result in the same and target being out of sync. configuration Errors: Improper and untanded configuration of replication products can cause descriptions This type of discropality doesn't show up in the replicate lags since from the replication product perpective it is forming as configured. Replication latency with assynchranous replication there well be a short lag between changes to the service database and delivery to those chang to the target Failure to Meet the Maximum latency requirement however can potentially violate service level agreement levels or data compriance requirements, user errors :- often target databases are treated to offload Quary processing from the source database This enable rich operational reporting without impacting the application running on stone source database Requirements for managing Data consisting : high speed , low impact data comparisons support for heleroganoous databases capability for handling large data volumes flexible options for Managing data comparent nonimally intrusive & support for line databases with constantly changing date comparson of only changed data in continous replication. comparison of huge table through automailed and Manual partiering Data comparison reports for auditing pwg Data security Easy to use, understand Configure, deploy and diagness 7 a) Fault. tolerand servites using replicated state Machine Key requirement : Make a service fault tolerant E.g : lock Manager, They - value system state Machines are a powerful approach to creating such services A state Machine - Has a stored state, and receives input - Makes state transitions on each input and may output some results - Transition and output must be determinist A replicated state Machine is a Rstrave Machine that is replicated on Multiple nodes. - All replicas must get exactly the same inputs - Replicated log! state Machine processes only committed inputs - Even if some of the nodes fail, state and output can be obtained from other hodey Replicated state machine Replicated state Machine based on replicated doo Example commands assign values to variables client concersus module consensus consenses n 3 Module n 3 y 7 Module x 3 y 7 y # z Log 3 log Z 3 = 3 2 log x< y-4 ye- 7 xx-2 22-2 x yea Ye- plan 2 2 xc- Leader follower follower leader - declares log record corrutted after it replicated at a Majority of nodes, update of state Machine at each replica happens only after log record has been committed uses of replicated state Machine peplicated state Machines Can be used is implement wide variety of servites input can specify operations with parameters - But operation Must be deterministin - Result of operation can be sent from any replica crets executed only when log records is committed in replicated log usually sent from leader which Knows which part of log is committed Example: fault towart lock manager state: lock table operations: lock requests and lock reals output grant, or rollback requests on deadlock centralized implementation is made fault tolerant by simply running it on a replicated state machine. Fault dolerent Key - value store state They - value storage state operations. get() and pul() are first logged operations executed when the long round is in committed state note even gde operations need to be processed via by Google spanner uses replicated state Machine to implement They value store Data is partitioned and each partners is replicated across multiple nodes. Peplicas of a portition form a some going with one node as leader operations indiated at leader and replicated to other hoder part-A 3. Advantages of storing Multiple relations in a single file: complex structures can be implemented through the DBMS, thus increasing performance Disaduantages of storing Multiple relation in a single file Increases the size and complexity of the DBMS I For the two dirk mirrored care, me assume A disk and B disk In order to lose data; A and B need to be failed at the same time. If A is already failed and within 100,000 hours B disk will fail, then data will be lost. The other case is B is already failed and within 100,000 hours A will fael and then data will be lost. For the first case, A disk is failed for 100 hours every 100, 000 hours. so in order to Make B to fa it will need 100, 00012/100 hours Because the other Care, the time is reduced to 100,000 162 100) 3. Advantage pata retrieval: computer- based system provide enhanced data reterenal techniques to retrieve data stored in files in easy and efficient way. Editing It is a easy to edit any information stored in computers in form of files. specific application programs or editing software Can be used for this purpose. Disadvantage * Data redundancy It is possible that the information May be duplicated in different in files. This same Leads to data redundanly results memory mastage. pata inconsistenty Because of data redundar it is possible that data May not be in consistant star 5. Map database Management system are software programs designed to efficiently store and recall spatial information They are widely used in localization and navigation, especially in automotive applications They are playing an increasingly important role in the emerging areas of location based services, active safety function and advanced driver - assistance system common to there functions is the requireMent for Clus on - board Map database that contains information describing the road network 2. Database indexing, Hash tables May also be used as disk - based data structures and database indicas (such as in dbM) although B- trees more popular in these applications in murlt:- - node database system hash tables are commonly used sto destribute rours amongst nodes, reducing network traffic enobles restrictions to be placed on reuse of 4. be mysol placed on reuse of previous passwords. To establish history password. - reuse policy globally, use the password and password reuse internal system variables Database Systoms 2127200881076 TB 2 cat-III Subcode IT13365 Part-C 8) b) Deadlock handling Dead lock prevention may result in unnecessary waiting and roll back. Furthermore, certain deadlock -prevention techniques May required more site to be involved in the environment execution of a transaction than would otherwise be the Case If we allowed deadlocks to occur and relay on deadlock detection, the Main problem in a distributed system is deciding how to maintain the wait for graph. T 2 T2) 4 Ta 3 5 3 sites, sites, 2 Common techniques for dealing with this issue require that each site keep a local wait for graph. adowh * The hodes of the graph correspond to all the transactions that are currently eithor hobling or requesting any of the items local to that site T T2 T-S 14 T5 3 13 depicts a system consisting of twosh for Maintainuse its local wait for graph. T2 and T3 appear in both graphs, indiating that the transaction have requested item at both sides 3 These local wait for graph are constructed in the usual manner for local transactions and do iteus R when a transaction T, on site S, needs a resource in site S2, it send S2, it sends a request Message to site S2.IF the resource is held by transaction TJ. the system insorts an edge Ti Tj in the local wait for graph of site S2. the centralized dead lock delation approach the systam Constructs and Maintains a global wait-for - graph in a single site The dead lock. - detection coordinator. of Since there is communication delay in the system we must distinguish between two types of wait- for exaphs. 4 of The real grouph describes the real but unknown state of the syctom at any instance in time, as would be seen by an emmiscient observer. 5 4. 13 S, S2 G 5 Coordinator whonever a new edgle is insected in or removed from one of the local wait- for graph Periodically , when a number of changes have occurred in a local wait for graph 5 xwhenever the coordinator needs to invoke the cycle delation algorithm. when the Coordinator invokes the deadlock detection algorithm it search has been selected as victim. This shane may produce roll backs if: False cycles: & It exist in the glabal wait For grouph As an illustration 1. Considered a snapshot of the system represented has the local wait- for grouph The T2 relogas the resource that it is holding wh site S1, resulting in the deletion of the edgo T, - Tz in S1. of A dead lock has inded occured and a victim has been picked, while one of the transactions was aborted For reason unrelated to the deadlack. *Both T2 and T3 are now rolled back although only T2 needed to be rolled back He Dood lock detation an be done is a distributed manner with several sites taking on parts of the task, instadd of its being done at ll siab site R the however, such algorithins are more complicated and more exepsive. Part-B (a) b) challenges in maintaining Data Consistancy. Data discrepance occrs when the data in the target database daiates Form the sourse database. The exetent to which the data derivate depends on various factors, , some of which you be intecled and other unitended 7 Lift E shift work load b cloud since the world is Moving towards cloud the lift & shift work lead from on - Premises to cloud is the good of today's IT world. Differences in source and tarent Differnes in / source and target dateshooe configuration. g locales, Indianness or database Instrantion Errors: Before Migration or replication can begin the target database (s) will need to be instratiated with the Correct schora and Contraints failure to doso will rouet in the Source and targ at being out of sync. of Configuration essors Improper and uninterested configuration of replication products can cause discropancea This MOU present Q A look for delative the issues as worl Requirements for honoalya Data consistency high spood low impact dats support for heteralling data bases minimally intrueivo * sero dountime of Source and larget x capability le ident data inconsistancy R low import on hardware and hellow & Data society Fazi to use understand, deplay and diagnose 9 a) Fault - tolerant services key requirement: make a Service Fault tolerant eg. lack Manager, Key valuestarge system # state machines are a powerful aproach to creating such services state machine & Has a stored state, and receives inputs to Makes state transitions on each input, and may output some results deterministic for Transitions and output must be replicated state machine K It has multiple nodes a Replicated lag state sachine process only committee inputs 8 Even if some of the nodes fail, state and output can be obtained from other nodes. to M Peplicated state machina based on replected log * rample commande assign values & variable Jion Het / breong H 3 Casses ve 3 Giorcus R3 Modulb Module of 47 rocheb gift 23 23 3 lops Heador Followor leader declara by record committed after it is replacated at a majarity of rodes, update of state Machine at sach vertica happens only after by records has been committed of Peplicated state machines Can be used le implement wide variety of services 11 R Inputs can specify operations with Parameters But operations Must be dotoministic t Rosult of operation can be sont from any replica gots executed only when lops record is committed in replicated lag usually sent form leader which knows which part of be is committed Eg: Fault Polesant - lack manger state = lock table go operations lock vea usts and locarelouse output grant , or roll back requestes on dead lock 12 Fg: Fault talerant Vers valio store state voys-valia storage S tate operation n got C) and Putcs are first lagged & operation S exceeded when to log records in committed state Gongle spanner uses replicated state machine to imploment Rev-value spere Replicas of a partition form a Paueas group with one node as leader operations initiated at bader, and replicated to other nodes Parl A For the two disk mirrored case , we assume A disk and B DISK. It order to lose data, A and B need to be failed at the same timo if A is alleaded and with in 100,000 hours B disk Rx will fail, then datawill be last The 0 ther case is B is alloady failed and within 100,000 lows A will fail and then data will be lost for the first call . 1 Disk is failed for 100 hours evory 100,000 hours. So in order to Male B to Fill if will need 100,00012/ 100 hours, Because the other COLD the time is reduced to 100) 00012/ C2 wo) 2) Data base indesing Hash tables May also be used as disk . based data structures and database indices although B- trees all more popular in these applications. It Multi-node databosystems, hash tables are Commonly used to distribute rows amongst nodes, reducing network haffic for bash joins. 3 Advantage Disadvantage Data retrival : Data Redundancy :- Computer- based It is possible that systems povide enhanced the same intermation data retrieval technique May Joe duplicated to retrieve data in different files stored in Files in easy this leads to data and efficient way redundary results in Editing Memory wastage It is say to edit Data inconsistency any information stored in Because of data redundancy, it is Computers in Form of Files Possiable that data may not be in Consistent stato 5 Map database Management. systems are software programs desinghed le efficiently store and roall spatial industria than are widely weed in localization and radigation especially in automatic applications. It plays important rate same in the emergica areas of location- based services , active safety Function and advanced driver assistance systems. common to these Functions is the requirement for an on board Map database that contains information decebbing the road network. SANTHOSH.M IT-B 2127200801078 IT 18305 - DATABASE SYSTEMS CAT-3 (IIIrd Semester) 11.02.22 Part - A 3) Advantages for stoling multiple relations in a single file: * complex structures can be implemented through the DBMS, thus increasing B performance. Disadvantages of storing multiple relations in a single file * Increases the singe and complexity of the DBMS. D For the two disk mirrored use we assume A disk and B disk In order to lose data, A and B need to be footed failed at the same time. If A is already failed and within 1000000 hours B disk will fail, then data will be lost The other case is B is already failed and within 100,000 hours A will fail, and them data will be lost. If the mean time to failure of a single disk is 100000 hours and the mean time to repair is 10 has then the mean time to dola loss is Clog; Doo ^ 2) 2 * 100. 5) Map database. Map database management systems all software proframs deigned to efficiently store and reall spatial information. is) 2) B+ trees can be used to brate block containing the search key. we choose B+ tree because it is a self- balaning tree data Structure that maintains data and allows searches, sequential auess, instructions and deletions. part -B 6)b) Challenges in Maintaining Data Consistently Data discryancy owurs when the data in the target database devites from the source database The extent to which the data deviates depends on various factors. Even when using products that replicate data data reliebly, such as Oracle, there remains potential causes of data discrepancy If the good of the Migration Errors Different kinds of miglation tools are employed to facilitate the initial load of the target databases before replication can begin Defferences in configulation for handling data by the miglation tools and replication products can result in data discrepances Lift & shift workload to Cloud Since the world is moving towards the lord, the lift & shift of database from on premises to loud is the held of today's IT world. Difference in Source and Target Diff in source and target database configination, for example different modings, Cordes, Indianess ol dobbese versions can cause subtle discrepanies to happen during miglation and replictions. Instantiation Errors Before migration or replication can begin, the target databases will need to be instantized with the correct schema and constraints. Configilation Errors Improper and unintended configuration of repliction products Can cause discrepansies. Gaps in Repliction Although repliction is hobled between source and target databases and is working perfectly well, there are instances where data insected on the source will not be replaced. Replication Latery inbetwen changes to the source database and delivery of dose with asymehranous replidion, there will be a sholt log changes to the farget. user errors often target databases are creded to offtoad query processing from the soure detabage This hobly rich operation reporting without impacting the applications running Application errors Applications that use target data bases can potentially change dot due to famly Rogic aswell as, application you upgrades. The technology requirements for manging data consistency are : High speed, low impact data comparisons. Support for hetaogenous databases. Capability for handling large data volumes. Flexible options for mangaing data comperisons. Support for live databases with outantly changing data. Minimally intersive. Comparison of only changed data in countinious replication Comparison of huge table through automated & manuel participang x Data compersion reports for auditing purposes x zero downtime of source and target systems. x Capability to identify data incount inconsistency Low impact on hardwide and network resources. Flexible regorting for verying & roles and access levels. XData Security * Easy to use, understand, configure deplay and diagnis Part C Da) Fault Tolerant Services using Replicated State Machines X Key requirement make a service fault tolerant. Eg : lock manager key-value storage system, State machines are a powerful approach to creating such servies. A state machine & Has a stored state, and received inspects. results Makes state transactions on each input, and may output some Transaction and output must be deterministri * A replicated state machines is a state machine that is replicted on multiple nodes. * All replies must get exactly the same inputs inputs. Replisted log. State machine processes only committed obtained from other nodes. Even if some of the holes fail, state and output can be Replicted State Machine * Replicated state onestine based on replicted los x Example commands assign values to varables thirt yo? consents xx Conserves x 3 module y module y 7 23 2 3 Log Log XELZ< 2 xaz yeh yey ZE2 X-2 y. xaz you ye of leader follower consensous x3 module yr 23 Leg xt2 ZE2 xk3 yea you) follower Leader dalaes los record committed after it is replisted as a majority of nodes. Uses of Replicted State Machines Replicated state machines can be used to implement wide variety of services Inputs can specify operations with palameters. But operations deterministic must be Result of operation cambe sent to from any replica. Gets weat executed only when log record is committed in replicated log. Example : fault tolerant lock manger state : lock table. operations : lick requests and lock releases Output : grant, or rollback requests on deadlock centralized implementation is made fault du telerant by simply running it on a replicated state machine. Fault tolerant key value- - store State: key value storage state Operations : get ( and put is are first logged Google spanner used uses replicated state machine to implement key value store. multiple hodes. Data is partitioned, and each partition is replicated across 8)b) Deadlock Handling The deadlake handling prevention and deadlak detection algorithms can be used in a distrubtited system, provided that modifications are made. For example we can use the tree proterol by defining a global tree among the system data items. Similarly, the timestamp. - ordering approach could be directly applied to a distributed environment. Deadlak prevention may result in unnecessery waiting and rollback Furthermore, certain deadlok -prevention techniques may require more sites to be involved in the execution of a transaction than would otherwise the case. If we allow deadlorks to ourh and rely on deadhrk detection, the main problem in a distributed system is deviding how to maintain the wait-for graph. T1 T 2 T25 4 T5 T3 T3 site S1 site S2 common techniques for dealing with the issue require that each site keeping a loal wait for graph. The nodes of the graph correspond to all the transactions Chrol as well as non load) that are currently either holding or requesting any of the items local to that site For example the above diagram deputs a system awarding consisting of two sites, each maistaining its local wait-for graph note that transactions T2 and T3 appear in both graphs, indicating that the transactions have requested items at both sites. These lord wait-for graphs are constructed in the used manner for lord transactions and data items. when a transaction Ti on site S, needs a resource in site S2, it sends a request message to site S2 If the resource is held by transaction Ti, the system inserts an edge Ti Ij in the loud wait -for graph of site S2. clearly, if any lord wait for graphs has a cycle, dealok has owned On the other hand, the fact that there are no cycles in any of the loud wait- for graphs does not mean that there are ho cycles in any of the lord wait- for graphs does does not mean there are no dealorks Each wait -for graph is anythie; neverthless, a deadlak exists in the system because the union of the loud wait- for graphs contains a cycle. In the centralized deadlack approach, the system construte and maintain a global wait -for graph in a single site: the deadlock -detection coordinator. Since there is communication delay in the system, we must distinguish between the two types of wait for graphs The real graph describes the real but unknonnown state of the system at any insurance instance in time, as would be seen by am amnisment observer. The constructed graph is an approximation generated by the controller during the Secution of the controller's algorithm Ti r2 T4 T5 3 T1 TI T2 T3 S, S2 Ti T3 T2 coordinator False cycles in the global wait-for graph The global watefor graph can be reconstructed or updated under these conditions:- the Whenever lord a new edge inhested in or removed from one of wait-for graphs. x Perididally, when a number of changes have ownered in a lord wait-for graph. * whenever the coordinator needs to mioke the lifele - -detection about when the coordinator whokes the deadlock - delthens detection algorithm it searches its global graph. If it finds a cycle, it selects a victim to be rolled back The coordinata must notify all the sites that a particular transaction has been selated as victim The sites in turn, roll back the victim transaction This scheme may produce unneressary rollbacks if : False Cycles exists in the global wait-for graph Asam illustration, consider a Snepshot of the system represented by the loral wait-for graphs. Suppose T2 releases the resource that it is holding in sites, resulting in the detection of the Edge Ti T2 in S1. Transaction T2 then requests a resource held by T3 at site S2, resulting in the addition of the edge T2 T34 S2 . If the hisert T2 T3 message from S2 arrives before the remove T1 2T2 message from Si, the coordinator may discover the false cycle T1 2 T2 T3 of ta the insert. Deadark recovery may be initi ited * A deadlock has indeed ownered and a viction has bean picked, white one of the transations was aborted for reasons unrelated to the deadlork.